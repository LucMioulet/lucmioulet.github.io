# Tools: AWS Athena

## Use case
Do you often find yourself opening parquet, csv, json files on a jupyter notebook with pandas/dask/pyspark to do some ETL, basic analysis work ?
Do you often complain about the time it takes you to get access, or find any data in your datalake, especially if this data is split in many places?
Then Athena is your dream come true.

[Athena](https://aws.amazon.com/athena) is a AWS service that lets you query s3 stored files (parquet, csv, orc...). 
Let's imagine that we have a bunch of parquet file containing the orders of all you customers in the last years.
You are interested in knowing how many customers you have per city in France.
With Athena you could write a query such as:

```
SELECT city, count(customers) FROM customers.address WHERE country='France' GROUP BY city;
```

Depending on the size of your data this could take from a couple of seconds to a couple of minutes.

## Cost management
The cost of athena is around 5$ per Terabyte searched. 
Of course it can become more expensive than managing your own EMR clusters/EC2 machine for certain tasks.
For simple analysis it can save you a lot of time, and time is money.

There is a simple "trick" can help you save a lot of money. 
Athena uses prestodb, therefore it works very well with partitioned data.
If you know in advance some partition keys, it is worth the time of organizing your data per partition.
In our previous example, let's say we are a huge company shipping all over the world (let's call ourselves Arazon).
You could partition your data in such a way:

```
address/country=France/city=Paris/0.part.parquet
                                /1.part.parquet
                      city=Bordeaux/0.part.parquet
                                /1.part.parquet
address/country=Italy/city=Rome/0.part.parquet
                                /1.part.parquet
                      /city=Milan/0.part.parquet
                                /1.part.parquet                                
```

By doing this partitioning the previous query would ignore any data present in the country=Italy folder.
Other tricks to reduce costs are compression and using columnar formats, see the [pricing documentation](https://aws.amazon.com/athena/pricing).

## Visualizations with athena
Athena does not give you the power to create visualizations.
You could however take advantage of its incredible power to crunch high volumes of data and load a small summary on your jupyter notebook to create a visualization with seaborn or plotly.
For this you need to use the package pyathena.

e.g:

```
from pyathena import connect
import pandas as pd

conn = connect(region_name='us-west-2', work_group='my-workgroup', s3_staging_dir='s3://bucket')
query="SELECT city, count(customers) FROM customers.address WHERE country='France' GROUP BY city"
df = pd.read_sql(query, conn)

```


## Limits
Athena is not extremely efficient with json. The deserializing has a cost which can make it prohibitive to use in with tens of thousands of json.
I would recommend using parquet, parquet being a columnar format, it is perfect for queries in which you are interested only in a subset of columns.

## Conclusion 
Athena has other features that can make data analysis much simpler.
The most powerful and potent one is likely its relationship with glue.
We will cover this in a following post.
